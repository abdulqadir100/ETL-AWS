[0m15:54:19.159075 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f88e8a96c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f88ea8b4d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f88ea8e9400>]}


============================== 15:54:19.166638 | 6d883958-3731-4180-851f-adddd361b957 ==============================
[0m15:54:19.166638 [info ] [MainThread]: Running with dbt=1.6.0
[0m15:54:19.167212 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/Users/abdulqadriafolabi/.dbt', 'log_path': '/Users/abdulqadriafolabi/Documents/DR/Data2bots/dbt_data2bots/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m15:54:19.441094 [error] [MainThread]: Encountered an error:
Runtime Error
  at path ['threads']: ['1 or more'] is not of type 'integer'
[0m15:54:19.442394 [debug] [MainThread]: Command `dbt run` failed at 15:54:19.441678 after 0.34 seconds
[0m15:54:19.442761 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f88e8a96c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f88eac069d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f88eac0d400>]}
[0m15:54:19.443131 [debug] [MainThread]: Flushing usage events
[0m15:54:40.789371 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb760716910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb7300c3c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb762281370>]}


============================== 15:54:40.795969 | e289d11d-69dd-468d-b74c-3c43478eedda ==============================
[0m15:54:40.795969 [info ] [MainThread]: Running with dbt=1.6.0
[0m15:54:40.796557 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/abdulqadriafolabi/Documents/DR/Data2bots/dbt_data2bots/logs', 'profiles_dir': '/Users/abdulqadriafolabi/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m15:54:40.928921 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e289d11d-69dd-468d-b74c-3c43478eedda', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb7621fb820>]}
[0m15:54:40.938104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e289d11d-69dd-468d-b74c-3c43478eedda', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb762357a00>]}
[0m15:54:40.938951 [info ] [MainThread]: Registered adapter: postgres=1.6.0
[0m15:54:40.997609 [debug] [MainThread]: checksum: 08dabea3fc65f9f8dd48613681c2bcd827397a7ba9ce4c6d52980aba1215e8c9, vars: {}, profile: , target: , version: 1.6.0
[0m15:54:40.998715 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m15:54:40.999183 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'e289d11d-69dd-468d-b74c-3c43478eedda', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb762364a30>]}
[0m15:54:41.740728 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'my_first_dbt_model' in the 'models' section of file 'models/abduafol4283_analytics/schema.yml'
[0m15:54:41.741877 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'my_second_dbt_model' in the 'models' section of file 'models/abduafol4283_analytics/schema.yml'
[0m15:54:41.758755 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_data2bots.unique_my_first_dbt_model_id.16e066b321' (models/abduafol4283_analytics/schema.yml) depends on a node named 'my_first_dbt_model' in package '' which was not found
[0m15:54:41.759310 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_data2bots.not_null_my_first_dbt_model_id.5fb22c2710' (models/abduafol4283_analytics/schema.yml) depends on a node named 'my_first_dbt_model' in package '' which was not found
[0m15:54:41.759667 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_data2bots.unique_my_second_dbt_model_id.57a0f8c493' (models/abduafol4283_analytics/schema.yml) depends on a node named 'my_second_dbt_model' in package '' which was not found
[0m15:54:41.760008 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_data2bots.not_null_my_second_dbt_model_id.151b76d778' (models/abduafol4283_analytics/schema.yml) depends on a node named 'my_second_dbt_model' in package '' which was not found
[0m15:54:41.790027 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e289d11d-69dd-468d-b74c-3c43478eedda', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb770b70310>]}
[0m15:54:41.802600 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e289d11d-69dd-468d-b74c-3c43478eedda', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb761029220>]}
[0m15:54:41.803195 [info ] [MainThread]: Found 4 models, 0 sources, 0 exposures, 0 metrics, 349 macros, 0 groups, 0 semantic models
[0m15:54:41.803793 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e289d11d-69dd-468d-b74c-3c43478eedda', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb7623649a0>]}
[0m15:54:41.805447 [info ] [MainThread]: 
[0m15:54:41.806320 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m15:54:41.807575 [debug] [ThreadPool]: Acquiring new postgres connection 'list_d2b_accessment'
[0m15:54:41.817581 [debug] [ThreadPool]: Using postgres connection "list_d2b_accessment"
[0m15:54:41.818123 [debug] [ThreadPool]: On list_d2b_accessment: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "connection_name": "list_d2b_accessment"} */

    select distinct nspname from pg_namespace
  
[0m15:54:41.818665 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:54:45.692174 [debug] [ThreadPool]: SQL status: SELECT 124 in 4.0 seconds
[0m15:54:45.696561 [debug] [ThreadPool]: On list_d2b_accessment: Close
[0m15:54:45.698783 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_d2b_accessment, now list_d2b_accessment_abduafol4283_analytics)
[0m15:54:45.704683 [debug] [ThreadPool]: Using postgres connection "list_d2b_accessment_abduafol4283_analytics"
[0m15:54:45.705060 [debug] [ThreadPool]: On list_d2b_accessment_abduafol4283_analytics: BEGIN
[0m15:54:45.705345 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:54:49.181261 [debug] [ThreadPool]: SQL status: BEGIN in 3.0 seconds
[0m15:54:49.183646 [debug] [ThreadPool]: Using postgres connection "list_d2b_accessment_abduafol4283_analytics"
[0m15:54:49.184777 [debug] [ThreadPool]: On list_d2b_accessment_abduafol4283_analytics: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "connection_name": "list_d2b_accessment_abduafol4283_analytics"} */
select
      'd2b_accessment' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'abduafol4283_analytics'
    union all
    select
      'd2b_accessment' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'abduafol4283_analytics'
    union all
    select
      'd2b_accessment' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'abduafol4283_analytics'
  
[0m15:54:49.364926 [debug] [ThreadPool]: SQL status: SELECT 0 in 0.0 seconds
[0m15:54:49.367777 [debug] [ThreadPool]: On list_d2b_accessment_abduafol4283_analytics: ROLLBACK
[0m15:54:49.559714 [debug] [ThreadPool]: On list_d2b_accessment_abduafol4283_analytics: Close
[0m15:54:49.566721 [debug] [MainThread]: Using postgres connection "master"
[0m15:54:49.567273 [debug] [MainThread]: On master: BEGIN
[0m15:54:49.567669 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:54:52.260901 [debug] [MainThread]: SQL status: BEGIN in 3.0 seconds
[0m15:54:52.266934 [debug] [MainThread]: Using postgres connection "master"
[0m15:54:52.268004 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v', 'm')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m15:54:52.564694 [debug] [MainThread]: SQL status: SELECT 61 in 0.0 seconds
[0m15:54:52.577340 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e289d11d-69dd-468d-b74c-3c43478eedda', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb770b9a8e0>]}
[0m15:54:52.579903 [debug] [MainThread]: On master: ROLLBACK
[0m15:54:52.791521 [debug] [MainThread]: Using postgres connection "master"
[0m15:54:52.792796 [debug] [MainThread]: On master: BEGIN
[0m15:54:53.296419 [debug] [MainThread]: SQL status: BEGIN in 1.0 seconds
[0m15:54:53.298015 [debug] [MainThread]: On master: COMMIT
[0m15:54:53.298857 [debug] [MainThread]: Using postgres connection "master"
[0m15:54:53.299596 [debug] [MainThread]: On master: COMMIT
[0m15:54:53.560466 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m15:54:53.562224 [debug] [MainThread]: On master: Close
[0m15:54:53.566224 [info ] [MainThread]: Concurrency: 5 threads (target='prod')
[0m15:54:53.567390 [info ] [MainThread]: 
[0m15:54:53.579723 [debug] [Thread-1  ]: Began running node model.dbt_data2bots.agg_public_holiday
[0m15:54:53.580543 [debug] [Thread-2  ]: Began running node model.dbt_data2bots.late_shp
[0m15:54:53.581750 [info ] [Thread-1  ]: 1 of 2 START sql table model abduafol4283_analytics.agg_public_holiday ......... [RUN]
[0m15:54:53.582950 [info ] [Thread-2  ]: 2 of 2 START sql table model abduafol4283_analytics.late_shp ................... [RUN]
[0m15:54:53.584313 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_d2b_accessment_abduafol4283_analytics, now model.dbt_data2bots.agg_public_holiday)
[0m15:54:53.585358 [debug] [Thread-2  ]: Acquiring new postgres connection 'model.dbt_data2bots.late_shp'
[0m15:54:53.585963 [debug] [Thread-1  ]: Began compiling node model.dbt_data2bots.agg_public_holiday
[0m15:54:53.586481 [debug] [Thread-2  ]: Began compiling node model.dbt_data2bots.late_shp
[0m15:54:53.606291 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_data2bots.agg_public_holiday"
[0m15:54:53.611763 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_data2bots.late_shp"
[0m15:54:53.614021 [debug] [Thread-1  ]: Timing info for model.dbt_data2bots.agg_public_holiday (compile): 15:54:53.586903 => 15:54:53.613465
[0m15:54:53.614749 [debug] [Thread-2  ]: Timing info for model.dbt_data2bots.late_shp (compile): 15:54:53.606873 => 15:54:53.614467
[0m15:54:53.615386 [debug] [Thread-1  ]: Began executing node model.dbt_data2bots.agg_public_holiday
[0m15:54:53.615962 [debug] [Thread-2  ]: Began executing node model.dbt_data2bots.late_shp
[0m15:54:53.674749 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbt_data2bots.agg_public_holiday"
[0m15:54:53.675217 [debug] [Thread-2  ]: Writing runtime sql for node "model.dbt_data2bots.late_shp"
[0m15:54:53.676342 [debug] [Thread-2  ]: Using postgres connection "model.dbt_data2bots.late_shp"
[0m15:54:53.676861 [debug] [Thread-1  ]: Using postgres connection "model.dbt_data2bots.agg_public_holiday"
[0m15:54:53.677279 [debug] [Thread-2  ]: On model.dbt_data2bots.late_shp: BEGIN
[0m15:54:53.677633 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: BEGIN
[0m15:54:53.678001 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m15:54:53.678339 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:54:57.235976 [debug] [Thread-2  ]: SQL status: BEGIN in 4.0 seconds
[0m15:54:57.237125 [debug] [Thread-2  ]: Using postgres connection "model.dbt_data2bots.late_shp"
[0m15:54:57.237817 [debug] [Thread-2  ]: On model.dbt_data2bots.late_shp: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.late_shp"} */

  
    

  create  table "d2b_accessment"."abduafol4283_analytics"."late_shp__dbt_tmp"
  
  
    as
  
  (
    with base_shipment as (
 select shipment_id,
	order_id,
	date(shipment_date) as shipment_date,  -- convert to a date column from text 
	date(delivery_date) as delivery_date  -- convert to a date column from text 
	
from abduafol4283_staging.shipment_deliveries
),

base_order as (
	select 	order_id,
			date(order_date) as order_date  -- convert to a date column from text 
	from abduafol4283_staging.orders
),

order_shipments as (
	select base_shipment.shipment_id,
			base_shipment.order_id,
	base_shipment.shipment_date,
	base_shipment.delivery_date,
	base_order.order_date,
	(date(base_order.order_date) + INTERVAL '6 days')::date as delivery_due_date   -- create a column for the delivery due date as 6 days after order_date
	
	from base_shipment 
	left join base_order
	on base_shipment.order_id = base_order.order_id
 
),
late_shipment as (
select *
from order_shipments
where shipment_date >= delivery_due_date)

select count(1)   --538
from late_shipment
  );
  
[0m15:54:57.884119 [debug] [Thread-2  ]: SQL status: SELECT 1 in 1.0 seconds
[0m15:54:57.892675 [debug] [Thread-2  ]: Using postgres connection "model.dbt_data2bots.late_shp"
[0m15:54:57.893248 [debug] [Thread-2  ]: On model.dbt_data2bots.late_shp: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.late_shp"} */
alter table "d2b_accessment"."abduafol4283_analytics"."late_shp__dbt_tmp" rename to "late_shp"
[0m15:54:58.071612 [debug] [Thread-2  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m15:54:58.091416 [debug] [Thread-2  ]: On model.dbt_data2bots.late_shp: COMMIT
[0m15:54:58.092129 [debug] [Thread-2  ]: Using postgres connection "model.dbt_data2bots.late_shp"
[0m15:54:58.092492 [debug] [Thread-2  ]: On model.dbt_data2bots.late_shp: COMMIT
[0m15:54:58.232586 [debug] [Thread-2  ]: SQL status: COMMIT in 0.0 seconds
[0m15:54:58.233110 [debug] [Thread-1  ]: SQL status: BEGIN in 5.0 seconds
[0m15:54:58.242637 [debug] [Thread-2  ]: Using postgres connection "model.dbt_data2bots.late_shp"
[0m15:54:58.243161 [debug] [Thread-1  ]: Using postgres connection "model.dbt_data2bots.agg_public_holiday"
[0m15:54:58.243530 [debug] [Thread-2  ]: On model.dbt_data2bots.late_shp: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.late_shp"} */
drop table if exists "d2b_accessment"."abduafol4283_analytics"."late_shp__dbt_backup" cascade
[0m15:54:58.243967 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.agg_public_holiday"} */

  
    

  create  table "d2b_accessment"."abduafol4283_analytics"."agg_public_holiday__dbt_tmp"
  
  
    as
  
  (
    with base_order as (
	select 	order_id,
			customer_id,
			date(order_date) as order_date  -- convert from text to date	
	from abduafol4283_staging.orders
),

base_date as (
	select 	calendar_dt,
			day_of_the_week_num,
			month_of_the_year_num,
			working_day
	from if_common.dim_dates
),
	
orders_date as (
select *
from base_order
left join base_date
on base_order.order_date = base_date.calendar_dt
where base_date.day_of_the_week_num  between 1 and 5
and working_day = false 
),

orders_count_agg as (
	select 	month_of_the_year_num month_,
			count(1) num_hol_orders
	from orders_date
	group by month_of_the_year_num
)
	
select 
		current_date as ingestion_date,
		max(case when month_ = 1 then num_hol_orders else 0 end )as tt_order_hol_jan,
		max(case when month_ = 2 then num_hol_orders else 0 end) as tt_order_hol_feb,
		max(case when month_ = 3 then num_hol_orders else 0 end) as tt_order_hol_mar,
		max(case when month_ = 4 then num_hol_orders else 0 end) as tt_order_hol_apr,
		max(case when month_ = 5 then num_hol_orders else 0 end) as tt_order_hol_may,
		max(case when month_ = 6 then num_hol_orders else 0 end) as tt_order_hol_jun,
		max(case when month_ = 7 then num_hol_orders else 0 end) as tt_order_hol_jul,
		max(case when month_ = 8 then num_hol_orders else 0 end) as tt_order_hol_aug,
		max(case when month_ = 9 then num_hol_orders else 0 end) as tt_order_hol_sep,
		max(case when month_ = 10 then num_hol_orders else 0 end) as tt_order_hol_oct,
		max(case when month_ = 11 then num_hol_orders else 0 end) as tt_order_hol_nov,
		max(case when month_ = 12 then num_hol_orders else 0 end) as tt_order_hol_dec
from orders_count_agg
  );
  
[0m15:54:58.396078 [debug] [Thread-2  ]: SQL status: DROP TABLE in 0.0 seconds
[0m15:54:58.398249 [debug] [Thread-2  ]: Timing info for model.dbt_data2bots.late_shp (execute): 15:54:53.640631 => 15:54:58.398034
[0m15:54:58.398747 [debug] [Thread-2  ]: On model.dbt_data2bots.late_shp: Close
[0m15:54:58.399823 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e289d11d-69dd-468d-b74c-3c43478eedda', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb730546bb0>]}
[0m15:54:58.400748 [info ] [Thread-2  ]: 2 of 2 OK created sql table model abduafol4283_analytics.late_shp .............. [[32mSELECT 1[0m in 4.81s]
[0m15:54:58.401397 [debug] [Thread-2  ]: Finished running node model.dbt_data2bots.late_shp
[0m15:54:58.477854 [debug] [Thread-1  ]: SQL status: SELECT 1 in 0.0 seconds
[0m15:54:58.482814 [debug] [Thread-1  ]: Using postgres connection "model.dbt_data2bots.agg_public_holiday"
[0m15:54:58.483345 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.agg_public_holiday"} */
alter table "d2b_accessment"."abduafol4283_analytics"."agg_public_holiday__dbt_tmp" rename to "agg_public_holiday"
[0m15:54:58.694889 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m15:54:58.698436 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: COMMIT
[0m15:54:58.698889 [debug] [Thread-1  ]: Using postgres connection "model.dbt_data2bots.agg_public_holiday"
[0m15:54:58.699286 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: COMMIT
[0m15:54:58.867922 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
[0m15:54:58.874014 [debug] [Thread-1  ]: Using postgres connection "model.dbt_data2bots.agg_public_holiday"
[0m15:54:58.874592 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.agg_public_holiday"} */
drop table if exists "d2b_accessment"."abduafol4283_analytics"."agg_public_holiday__dbt_backup" cascade
[0m15:54:59.063997 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
[0m15:54:59.066029 [debug] [Thread-1  ]: Timing info for model.dbt_data2bots.agg_public_holiday (execute): 15:54:53.616384 => 15:54:59.065825
[0m15:54:59.066492 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: Close
[0m15:54:59.067716 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e289d11d-69dd-468d-b74c-3c43478eedda', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb770bbb160>]}
[0m15:54:59.068371 [info ] [Thread-1  ]: 1 of 2 OK created sql table model abduafol4283_analytics.agg_public_holiday .... [[32mSELECT 1[0m in 5.48s]
[0m15:54:59.068981 [debug] [Thread-1  ]: Finished running node model.dbt_data2bots.agg_public_holiday
[0m15:54:59.071434 [debug] [MainThread]: Using postgres connection "master"
[0m15:54:59.071948 [debug] [MainThread]: On master: BEGIN
[0m15:54:59.072271 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:55:01.212618 [debug] [MainThread]: SQL status: BEGIN in 2.0 seconds
[0m15:55:01.214037 [debug] [MainThread]: On master: COMMIT
[0m15:55:01.214765 [debug] [MainThread]: Using postgres connection "master"
[0m15:55:01.215261 [debug] [MainThread]: On master: COMMIT
[0m15:55:01.349046 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m15:55:01.350833 [debug] [MainThread]: On master: Close
[0m15:55:01.352851 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:55:01.353617 [debug] [MainThread]: Connection 'model.dbt_data2bots.agg_public_holiday' was properly closed.
[0m15:55:01.354296 [debug] [MainThread]: Connection 'model.dbt_data2bots.late_shp' was properly closed.
[0m15:55:01.355165 [info ] [MainThread]: 
[0m15:55:01.356050 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 19.55 seconds (19.55s).
[0m15:55:01.357305 [debug] [MainThread]: Command end result
[0m15:55:01.373591 [info ] [MainThread]: 
[0m15:55:01.374123 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:55:01.374437 [info ] [MainThread]: 
[0m15:55:01.374807 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m15:55:01.375670 [debug] [MainThread]: Command `dbt run` succeeded at 15:55:01.375573 after 20.62 seconds
[0m15:55:01.376112 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb760716910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb761029220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb762357820>]}
[0m15:55:01.376504 [debug] [MainThread]: Flushing usage events
[0m15:56:11.926930 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff5089b6c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff528773cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff4e81c03d0>]}


============================== 15:56:11.933613 | c3fafaa1-b1ac-4e1c-a4d0-102bb4c2c403 ==============================
[0m15:56:11.933613 [info ] [MainThread]: Running with dbt=1.6.0
[0m15:56:11.934180 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/abdulqadriafolabi/Documents/DR/Data2bots/dbt_data2bots/logs', 'fail_fast': 'False', 'profiles_dir': '/Users/abdulqadriafolabi/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m15:56:12.066699 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c3fafaa1-b1ac-4e1c-a4d0-102bb4c2c403', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff508125e50>]}
[0m15:56:12.075597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c3fafaa1-b1ac-4e1c-a4d0-102bb4c2c403', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff50a5c6a60>]}
[0m15:56:12.076242 [info ] [MainThread]: Registered adapter: postgres=1.6.0
[0m15:56:12.097016 [debug] [MainThread]: checksum: 08dabea3fc65f9f8dd48613681c2bcd827397a7ba9ce4c6d52980aba1215e8c9, vars: {}, profile: , target: , version: 1.6.0
[0m15:56:12.131332 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:56:12.132070 [debug] [MainThread]: Partial parsing: updated file: dbt_data2bots://models/abduafol4283_analytics/schema.yml
[0m15:56:12.150507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c3fafaa1-b1ac-4e1c-a4d0-102bb4c2c403', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff51830ffd0>]}
[0m15:56:12.161304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c3fafaa1-b1ac-4e1c-a4d0-102bb4c2c403', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff5182dee50>]}
[0m15:56:12.161835 [info ] [MainThread]: Found 4 models, 0 sources, 0 exposures, 0 metrics, 349 macros, 0 groups, 0 semantic models
[0m15:56:12.162209 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c3fafaa1-b1ac-4e1c-a4d0-102bb4c2c403', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff5182dee80>]}
[0m15:56:12.163686 [info ] [MainThread]: 
[0m15:56:12.164389 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m15:56:12.165483 [debug] [ThreadPool]: Acquiring new postgres connection 'list_d2b_accessment'
[0m15:56:12.175609 [debug] [ThreadPool]: Using postgres connection "list_d2b_accessment"
[0m15:56:12.176135 [debug] [ThreadPool]: On list_d2b_accessment: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "connection_name": "list_d2b_accessment"} */

    select distinct nspname from pg_namespace
  
[0m15:56:12.176452 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:56:14.859535 [debug] [ThreadPool]: SQL status: SELECT 124 in 3.0 seconds
[0m15:56:14.867730 [debug] [ThreadPool]: On list_d2b_accessment: Close
[0m15:56:14.872108 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_d2b_accessment, now list_d2b_accessment_abduafol4283_analytics)
[0m15:56:14.882807 [debug] [ThreadPool]: Using postgres connection "list_d2b_accessment_abduafol4283_analytics"
[0m15:56:14.883367 [debug] [ThreadPool]: On list_d2b_accessment_abduafol4283_analytics: BEGIN
[0m15:56:14.883841 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:56:17.709102 [debug] [ThreadPool]: SQL status: BEGIN in 3.0 seconds
[0m15:56:17.710647 [debug] [ThreadPool]: Using postgres connection "list_d2b_accessment_abduafol4283_analytics"
[0m15:56:17.711533 [debug] [ThreadPool]: On list_d2b_accessment_abduafol4283_analytics: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "connection_name": "list_d2b_accessment_abduafol4283_analytics"} */
select
      'd2b_accessment' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'abduafol4283_analytics'
    union all
    select
      'd2b_accessment' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'abduafol4283_analytics'
    union all
    select
      'd2b_accessment' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'abduafol4283_analytics'
  
[0m15:56:17.947441 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.0 seconds
[0m15:56:17.951746 [debug] [ThreadPool]: On list_d2b_accessment_abduafol4283_analytics: ROLLBACK
[0m15:56:18.920402 [debug] [ThreadPool]: On list_d2b_accessment_abduafol4283_analytics: Close
[0m15:56:18.935701 [debug] [MainThread]: Using postgres connection "master"
[0m15:56:18.936416 [debug] [MainThread]: On master: BEGIN
[0m15:56:18.936928 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:56:21.857540 [debug] [MainThread]: SQL status: BEGIN in 3.0 seconds
[0m15:56:21.858762 [debug] [MainThread]: Using postgres connection "master"
[0m15:56:21.859527 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v', 'm')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m15:56:22.334810 [debug] [MainThread]: SQL status: SELECT 61 in 0.0 seconds
[0m15:56:22.341543 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c3fafaa1-b1ac-4e1c-a4d0-102bb4c2c403', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff5182de340>]}
[0m15:56:22.342495 [debug] [MainThread]: On master: ROLLBACK
[0m15:56:22.850314 [debug] [MainThread]: Using postgres connection "master"
[0m15:56:22.851621 [debug] [MainThread]: On master: BEGIN
[0m15:56:23.288619 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
[0m15:56:23.294882 [debug] [MainThread]: On master: COMMIT
[0m15:56:23.295752 [debug] [MainThread]: Using postgres connection "master"
[0m15:56:23.296300 [debug] [MainThread]: On master: COMMIT
[0m15:56:23.937631 [debug] [MainThread]: SQL status: COMMIT in 1.0 seconds
[0m15:56:23.939909 [debug] [MainThread]: On master: Close
[0m15:56:23.942595 [info ] [MainThread]: Concurrency: 5 threads (target='prod')
[0m15:56:23.943701 [info ] [MainThread]: 
[0m15:56:23.948097 [debug] [Thread-1  ]: Began running node model.dbt_data2bots.agg_public_holiday
[0m15:56:23.948691 [debug] [Thread-2  ]: Began running node model.dbt_data2bots.late_shp
[0m15:56:23.949326 [info ] [Thread-1  ]: 1 of 2 START sql table model abduafol4283_analytics.agg_public_holiday ......... [RUN]
[0m15:56:23.949974 [info ] [Thread-2  ]: 2 of 2 START sql table model abduafol4283_analytics.late_shp ................... [RUN]
[0m15:56:23.950828 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_d2b_accessment_abduafol4283_analytics, now model.dbt_data2bots.agg_public_holiday)
[0m15:56:23.951696 [debug] [Thread-2  ]: Acquiring new postgres connection 'model.dbt_data2bots.late_shp'
[0m15:56:23.952261 [debug] [Thread-1  ]: Began compiling node model.dbt_data2bots.agg_public_holiday
[0m15:56:23.952682 [debug] [Thread-2  ]: Began compiling node model.dbt_data2bots.late_shp
[0m15:56:23.961687 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_data2bots.agg_public_holiday"
[0m15:56:23.964157 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_data2bots.late_shp"
[0m15:56:23.965703 [debug] [Thread-2  ]: Timing info for model.dbt_data2bots.late_shp (compile): 15:56:23.962267 => 15:56:23.965407
[0m15:56:23.966473 [debug] [Thread-1  ]: Timing info for model.dbt_data2bots.agg_public_holiday (compile): 15:56:23.953003 => 15:56:23.966063
[0m15:56:23.967228 [debug] [Thread-2  ]: Began executing node model.dbt_data2bots.late_shp
[0m15:56:23.967869 [debug] [Thread-1  ]: Began executing node model.dbt_data2bots.agg_public_holiday
[0m15:56:24.031425 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbt_data2bots.agg_public_holiday"
[0m15:56:24.031948 [debug] [Thread-2  ]: Writing runtime sql for node "model.dbt_data2bots.late_shp"
[0m15:56:24.035790 [debug] [Thread-2  ]: Using postgres connection "model.dbt_data2bots.late_shp"
[0m15:56:24.036167 [debug] [Thread-1  ]: Using postgres connection "model.dbt_data2bots.agg_public_holiday"
[0m15:56:24.036527 [debug] [Thread-2  ]: On model.dbt_data2bots.late_shp: BEGIN
[0m15:56:24.036873 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: BEGIN
[0m15:56:24.037209 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m15:56:24.037539 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:56:26.867294 [debug] [Thread-1  ]: SQL status: BEGIN in 3.0 seconds
[0m15:56:26.868744 [debug] [Thread-2  ]: SQL status: BEGIN in 3.0 seconds
[0m15:56:26.869820 [debug] [Thread-1  ]: Using postgres connection "model.dbt_data2bots.agg_public_holiday"
[0m15:56:26.870855 [debug] [Thread-2  ]: Using postgres connection "model.dbt_data2bots.late_shp"
[0m15:56:26.872412 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.agg_public_holiday"} */

  
    

  create  table "d2b_accessment"."abduafol4283_analytics"."agg_public_holiday__dbt_tmp"
  
  
    as
  
  (
    with base_order as (
	select 	order_id,
			customer_id,
			date(order_date) as order_date  -- convert from text to date	
	from abduafol4283_staging.orders
),

base_date as (
	select 	calendar_dt,
			day_of_the_week_num,
			month_of_the_year_num,
			working_day
	from if_common.dim_dates
),
	
orders_date as (
select *
from base_order
left join base_date
on base_order.order_date = base_date.calendar_dt
where base_date.day_of_the_week_num  between 1 and 5
and working_day = false 
),

orders_count_agg as (
	select 	month_of_the_year_num month_,
			count(1) num_hol_orders
	from orders_date
	group by month_of_the_year_num
)
	
select 
		current_date as ingestion_date,
		max(case when month_ = 1 then num_hol_orders else 0 end )as tt_order_hol_jan,
		max(case when month_ = 2 then num_hol_orders else 0 end) as tt_order_hol_feb,
		max(case when month_ = 3 then num_hol_orders else 0 end) as tt_order_hol_mar,
		max(case when month_ = 4 then num_hol_orders else 0 end) as tt_order_hol_apr,
		max(case when month_ = 5 then num_hol_orders else 0 end) as tt_order_hol_may,
		max(case when month_ = 6 then num_hol_orders else 0 end) as tt_order_hol_jun,
		max(case when month_ = 7 then num_hol_orders else 0 end) as tt_order_hol_jul,
		max(case when month_ = 8 then num_hol_orders else 0 end) as tt_order_hol_aug,
		max(case when month_ = 9 then num_hol_orders else 0 end) as tt_order_hol_sep,
		max(case when month_ = 10 then num_hol_orders else 0 end) as tt_order_hol_oct,
		max(case when month_ = 11 then num_hol_orders else 0 end) as tt_order_hol_nov,
		max(case when month_ = 12 then num_hol_orders else 0 end) as tt_order_hol_dec
from orders_count_agg
  );
  
[0m15:56:26.874066 [debug] [Thread-2  ]: On model.dbt_data2bots.late_shp: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.late_shp"} */

  
    

  create  table "d2b_accessment"."abduafol4283_analytics"."late_shp__dbt_tmp"
  
  
    as
  
  (
    with base_shipment as (
 select shipment_id,
	order_id,
	date(shipment_date) as shipment_date,  -- convert to a date column from text 
	date(delivery_date) as delivery_date  -- convert to a date column from text 
	
from abduafol4283_staging.shipment_deliveries
),

base_order as (
	select 	order_id,
			date(order_date) as order_date  -- convert to a date column from text 
	from abduafol4283_staging.orders
),

order_shipments as (
	select base_shipment.shipment_id,
			base_shipment.order_id,
	base_shipment.shipment_date,
	base_shipment.delivery_date,
	base_order.order_date,
	(date(base_order.order_date) + INTERVAL '6 days')::date as delivery_due_date   -- create a column for the delivery due date as 6 days after order_date
	
	from base_shipment 
	left join base_order
	on base_shipment.order_id = base_order.order_id
 
),
late_shipment as (
select *
from order_shipments
where shipment_date >= delivery_due_date)

select count(1)   --538
from late_shipment
  );
  
[0m15:56:27.079448 [debug] [Thread-1  ]: SQL status: SELECT 1 in 0.0 seconds
[0m15:56:27.096341 [debug] [Thread-2  ]: SQL status: SELECT 1 in 0.0 seconds
[0m15:56:27.119307 [debug] [Thread-1  ]: Using postgres connection "model.dbt_data2bots.agg_public_holiday"
[0m15:56:27.118053 [debug] [Thread-2  ]: Using postgres connection "model.dbt_data2bots.late_shp"
[0m15:56:27.120415 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.agg_public_holiday"} */
alter table "d2b_accessment"."abduafol4283_analytics"."agg_public_holiday" rename to "agg_public_holiday__dbt_backup"
[0m15:56:27.121204 [debug] [Thread-2  ]: On model.dbt_data2bots.late_shp: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.late_shp"} */
alter table "d2b_accessment"."abduafol4283_analytics"."late_shp" rename to "late_shp__dbt_backup"
[0m15:56:27.331449 [debug] [Thread-2  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m15:56:27.333924 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m15:56:27.357718 [debug] [Thread-2  ]: Using postgres connection "model.dbt_data2bots.late_shp"
[0m15:56:27.361410 [debug] [Thread-1  ]: Using postgres connection "model.dbt_data2bots.agg_public_holiday"
[0m15:56:27.361839 [debug] [Thread-2  ]: On model.dbt_data2bots.late_shp: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.late_shp"} */
alter table "d2b_accessment"."abduafol4283_analytics"."late_shp__dbt_tmp" rename to "late_shp"
[0m15:56:27.362345 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.agg_public_holiday"} */
alter table "d2b_accessment"."abduafol4283_analytics"."agg_public_holiday__dbt_tmp" rename to "agg_public_holiday"
[0m15:56:27.582057 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m15:56:27.584216 [debug] [Thread-2  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m15:56:27.640451 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: COMMIT
[0m15:56:27.641871 [debug] [Thread-2  ]: On model.dbt_data2bots.late_shp: COMMIT
[0m15:56:27.642564 [debug] [Thread-1  ]: Using postgres connection "model.dbt_data2bots.agg_public_holiday"
[0m15:56:27.643080 [debug] [Thread-2  ]: Using postgres connection "model.dbt_data2bots.late_shp"
[0m15:56:27.643546 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: COMMIT
[0m15:56:27.644052 [debug] [Thread-2  ]: On model.dbt_data2bots.late_shp: COMMIT
[0m15:56:27.858743 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
[0m15:56:27.874347 [debug] [Thread-2  ]: SQL status: COMMIT in 0.0 seconds
[0m15:56:27.902553 [debug] [Thread-2  ]: Using postgres connection "model.dbt_data2bots.late_shp"
[0m15:56:27.903304 [debug] [Thread-1  ]: Using postgres connection "model.dbt_data2bots.agg_public_holiday"
[0m15:56:27.903847 [debug] [Thread-2  ]: On model.dbt_data2bots.late_shp: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.late_shp"} */
drop table if exists "d2b_accessment"."abduafol4283_analytics"."late_shp__dbt_backup" cascade
[0m15:56:27.904235 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.agg_public_holiday"} */
drop table if exists "d2b_accessment"."abduafol4283_analytics"."agg_public_holiday__dbt_backup" cascade
[0m15:56:28.571544 [debug] [Thread-1  ]: SQL status: DROP TABLE in 1.0 seconds
[0m15:56:28.574606 [debug] [Thread-2  ]: SQL status: DROP TABLE in 1.0 seconds
[0m15:56:28.599570 [debug] [Thread-1  ]: Timing info for model.dbt_data2bots.agg_public_holiday (execute): 15:56:23.992001 => 15:56:28.598848
[0m15:56:28.601591 [debug] [Thread-2  ]: Timing info for model.dbt_data2bots.late_shp (execute): 15:56:23.968277 => 15:56:28.601388
[0m15:56:28.612002 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: Close
[0m15:56:28.612940 [debug] [Thread-2  ]: On model.dbt_data2bots.late_shp: Close
[0m15:56:28.614789 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3fafaa1-b1ac-4e1c-a4d0-102bb4c2c403', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff528a00eb0>]}
[0m15:56:28.616229 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3fafaa1-b1ac-4e1c-a4d0-102bb4c2c403', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff528a217f0>]}
[0m15:56:28.618196 [info ] [Thread-1  ]: 1 of 2 OK created sql table model abduafol4283_analytics.agg_public_holiday .... [[32mSELECT 1[0m in 4.66s]
[0m15:56:28.620136 [info ] [Thread-2  ]: 2 of 2 OK created sql table model abduafol4283_analytics.late_shp .............. [[32mSELECT 1[0m in 4.66s]
[0m15:56:28.621703 [debug] [Thread-1  ]: Finished running node model.dbt_data2bots.agg_public_holiday
[0m15:56:28.624391 [debug] [Thread-2  ]: Finished running node model.dbt_data2bots.late_shp
[0m15:56:28.642077 [debug] [MainThread]: Using postgres connection "master"
[0m15:56:28.643190 [debug] [MainThread]: On master: BEGIN
[0m15:56:28.643978 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:56:32.203095 [debug] [MainThread]: SQL status: BEGIN in 4.0 seconds
[0m15:56:32.208728 [debug] [MainThread]: On master: COMMIT
[0m15:56:32.209676 [debug] [MainThread]: Using postgres connection "master"
[0m15:56:32.210633 [debug] [MainThread]: On master: COMMIT
[0m15:56:32.411843 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m15:56:32.413991 [debug] [MainThread]: On master: Close
[0m15:56:32.418061 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:56:32.421111 [debug] [MainThread]: Connection 'model.dbt_data2bots.agg_public_holiday' was properly closed.
[0m15:56:32.422274 [debug] [MainThread]: Connection 'model.dbt_data2bots.late_shp' was properly closed.
[0m15:56:32.423505 [info ] [MainThread]: 
[0m15:56:32.424683 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 20.26 seconds (20.26s).
[0m15:56:32.426830 [debug] [MainThread]: Command end result
[0m15:56:32.440248 [info ] [MainThread]: 
[0m15:56:32.440768 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:56:32.441109 [info ] [MainThread]: 
[0m15:56:32.441466 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m15:56:32.442126 [debug] [MainThread]: Command `dbt run` succeeded at 15:56:32.442033 after 20.56 seconds
[0m15:56:32.442520 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff5089b6c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff50a5c6a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff5182ca610>]}
[0m15:56:32.442888 [debug] [MainThread]: Flushing usage events
[0m15:57:03.601275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bf079ec70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bf2414d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8c00278400>]}


============================== 15:57:03.607521 | 32ba5cbc-debe-4d98-87b4-8e0ba2cee898 ==============================
[0m15:57:03.607521 [info ] [MainThread]: Running with dbt=1.6.0
[0m15:57:03.608074 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/abdulqadriafolabi/.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/Users/abdulqadriafolabi/Documents/DR/Data2bots/dbt_data2bots/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m15:57:03.739932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '32ba5cbc-debe-4d98-87b4-8e0ba2cee898', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bf00bcf40>]}
[0m15:57:03.749245 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '32ba5cbc-debe-4d98-87b4-8e0ba2cee898', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bf27a6a90>]}
[0m15:57:03.749981 [info ] [MainThread]: Registered adapter: postgres=1.6.0
[0m15:57:03.771366 [debug] [MainThread]: checksum: 08dabea3fc65f9f8dd48613681c2bcd827397a7ba9ce4c6d52980aba1215e8c9, vars: {}, profile: , target: , version: 1.6.0
[0m15:57:03.804390 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m15:57:03.805106 [debug] [MainThread]: Partial parsing: updated file: dbt_data2bots://models/abduafol4283_analytics/best_performing_product.sql
[0m15:57:03.805529 [debug] [MainThread]: Partial parsing: updated file: dbt_data2bots://models/abduafol4283_analytics/agg_shipments.sql
[0m15:57:03.847429 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '32ba5cbc-debe-4d98-87b4-8e0ba2cee898', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8c002ca0d0>]}
[0m15:57:03.859753 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '32ba5cbc-debe-4d98-87b4-8e0ba2cee898', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bf1345be0>]}
[0m15:57:03.860321 [info ] [MainThread]: Found 4 models, 0 sources, 0 exposures, 0 metrics, 349 macros, 0 groups, 0 semantic models
[0m15:57:03.860682 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '32ba5cbc-debe-4d98-87b4-8e0ba2cee898', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bd0892f70>]}
[0m15:57:03.862260 [info ] [MainThread]: 
[0m15:57:03.863008 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m15:57:03.864149 [debug] [ThreadPool]: Acquiring new postgres connection 'list_d2b_accessment'
[0m15:57:03.874110 [debug] [ThreadPool]: Using postgres connection "list_d2b_accessment"
[0m15:57:03.874762 [debug] [ThreadPool]: On list_d2b_accessment: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "connection_name": "list_d2b_accessment"} */

    select distinct nspname from pg_namespace
  
[0m15:57:03.875228 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:57:05.849561 [debug] [ThreadPool]: SQL status: SELECT 124 in 2.0 seconds
[0m15:57:05.858043 [debug] [ThreadPool]: On list_d2b_accessment: Close
[0m15:57:05.861759 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_d2b_accessment, now list_d2b_accessment_abduafol4283_analytics)
[0m15:57:05.868950 [debug] [ThreadPool]: Using postgres connection "list_d2b_accessment_abduafol4283_analytics"
[0m15:57:05.869417 [debug] [ThreadPool]: On list_d2b_accessment_abduafol4283_analytics: BEGIN
[0m15:57:05.869776 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:57:07.939627 [debug] [ThreadPool]: SQL status: BEGIN in 2.0 seconds
[0m15:57:07.942970 [debug] [ThreadPool]: Using postgres connection "list_d2b_accessment_abduafol4283_analytics"
[0m15:57:07.944064 [debug] [ThreadPool]: On list_d2b_accessment_abduafol4283_analytics: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "connection_name": "list_d2b_accessment_abduafol4283_analytics"} */
select
      'd2b_accessment' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'abduafol4283_analytics'
    union all
    select
      'd2b_accessment' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'abduafol4283_analytics'
    union all
    select
      'd2b_accessment' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'abduafol4283_analytics'
  
[0m15:57:08.271849 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.0 seconds
[0m15:57:08.277337 [debug] [ThreadPool]: On list_d2b_accessment_abduafol4283_analytics: ROLLBACK
[0m15:57:08.501598 [debug] [ThreadPool]: On list_d2b_accessment_abduafol4283_analytics: Close
[0m15:57:08.513198 [debug] [MainThread]: Using postgres connection "master"
[0m15:57:08.513997 [debug] [MainThread]: On master: BEGIN
[0m15:57:08.514612 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:57:10.850394 [debug] [MainThread]: SQL status: BEGIN in 2.0 seconds
[0m15:57:10.853892 [debug] [MainThread]: Using postgres connection "master"
[0m15:57:10.854635 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v', 'm')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m15:57:11.034911 [debug] [MainThread]: SQL status: SELECT 61 in 0.0 seconds
[0m15:57:11.041421 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '32ba5cbc-debe-4d98-87b4-8e0ba2cee898', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bf1260f70>]}
[0m15:57:11.042482 [debug] [MainThread]: On master: ROLLBACK
[0m15:57:11.703113 [debug] [MainThread]: Using postgres connection "master"
[0m15:57:11.704751 [debug] [MainThread]: On master: BEGIN
[0m15:57:12.142208 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
[0m15:57:12.142965 [debug] [MainThread]: On master: COMMIT
[0m15:57:12.143404 [debug] [MainThread]: Using postgres connection "master"
[0m15:57:12.143758 [debug] [MainThread]: On master: COMMIT
[0m15:57:12.359225 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m15:57:12.360136 [debug] [MainThread]: On master: Close
[0m15:57:12.361620 [info ] [MainThread]: Concurrency: 5 threads (target='prod')
[0m15:57:12.362106 [info ] [MainThread]: 
[0m15:57:12.366375 [debug] [Thread-1  ]: Began running node model.dbt_data2bots.agg_public_holiday
[0m15:57:12.367026 [debug] [Thread-2  ]: Began running node model.dbt_data2bots.agg_shipments
[0m15:57:12.367535 [debug] [Thread-3  ]: Began running node model.dbt_data2bots.best_performing_product
[0m15:57:12.368121 [debug] [Thread-4  ]: Began running node model.dbt_data2bots.late_shp
[0m15:57:12.368797 [info ] [Thread-1  ]: 1 of 4 START sql table model abduafol4283_analytics.agg_public_holiday ......... [RUN]
[0m15:57:12.370007 [info ] [Thread-2  ]: 2 of 4 START sql table model abduafol4283_analytics.agg_shipments .............. [RUN]
[0m15:57:12.370766 [info ] [Thread-3  ]: 3 of 4 START sql table model abduafol4283_analytics.best_performing_product .... [RUN]
[0m15:57:12.371337 [info ] [Thread-4  ]: 4 of 4 START sql table model abduafol4283_analytics.late_shp ................... [RUN]
[0m15:57:12.372248 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_d2b_accessment_abduafol4283_analytics, now model.dbt_data2bots.agg_public_holiday)
[0m15:57:12.373075 [debug] [Thread-2  ]: Acquiring new postgres connection 'model.dbt_data2bots.agg_shipments'
[0m15:57:12.373865 [debug] [Thread-3  ]: Acquiring new postgres connection 'model.dbt_data2bots.best_performing_product'
[0m15:57:12.374985 [debug] [Thread-4  ]: Acquiring new postgres connection 'model.dbt_data2bots.late_shp'
[0m15:57:12.375605 [debug] [Thread-1  ]: Began compiling node model.dbt_data2bots.agg_public_holiday
[0m15:57:12.376047 [debug] [Thread-2  ]: Began compiling node model.dbt_data2bots.agg_shipments
[0m15:57:12.376485 [debug] [Thread-3  ]: Began compiling node model.dbt_data2bots.best_performing_product
[0m15:57:12.376900 [debug] [Thread-4  ]: Began compiling node model.dbt_data2bots.late_shp
[0m15:57:12.384570 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_data2bots.agg_public_holiday"
[0m15:57:12.386606 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_data2bots.agg_shipments"
[0m15:57:12.388440 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbt_data2bots.best_performing_product"
[0m15:57:12.391271 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbt_data2bots.late_shp"
[0m15:57:12.392709 [debug] [Thread-2  ]: Timing info for model.dbt_data2bots.agg_shipments (compile): 15:57:12.385010 => 15:57:12.392390
[0m15:57:12.393197 [debug] [Thread-4  ]: Timing info for model.dbt_data2bots.late_shp (compile): 15:57:12.388724 => 15:57:12.393019
[0m15:57:12.393717 [debug] [Thread-1  ]: Timing info for model.dbt_data2bots.agg_public_holiday (compile): 15:57:12.377221 => 15:57:12.393479
[0m15:57:12.394120 [debug] [Thread-2  ]: Began executing node model.dbt_data2bots.agg_shipments
[0m15:57:12.394523 [debug] [Thread-3  ]: Timing info for model.dbt_data2bots.best_performing_product (compile): 15:57:12.386979 => 15:57:12.394354
[0m15:57:12.394915 [debug] [Thread-4  ]: Began executing node model.dbt_data2bots.late_shp
[0m15:57:12.395596 [debug] [Thread-1  ]: Began executing node model.dbt_data2bots.agg_public_holiday
[0m15:57:12.432691 [debug] [Thread-3  ]: Began executing node model.dbt_data2bots.best_performing_product
[0m15:57:12.434137 [debug] [Thread-2  ]: Writing runtime sql for node "model.dbt_data2bots.agg_shipments"
[0m15:57:12.438584 [debug] [Thread-4  ]: Writing runtime sql for node "model.dbt_data2bots.late_shp"
[0m15:57:12.442544 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbt_data2bots.agg_public_holiday"
[0m15:57:12.446674 [debug] [Thread-3  ]: Writing runtime sql for node "model.dbt_data2bots.best_performing_product"
[0m15:57:12.447678 [debug] [Thread-1  ]: Using postgres connection "model.dbt_data2bots.agg_public_holiday"
[0m15:57:12.448154 [debug] [Thread-4  ]: Using postgres connection "model.dbt_data2bots.late_shp"
[0m15:57:12.448510 [debug] [Thread-3  ]: Using postgres connection "model.dbt_data2bots.best_performing_product"
[0m15:57:12.448847 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: BEGIN
[0m15:57:12.449195 [debug] [Thread-2  ]: Using postgres connection "model.dbt_data2bots.agg_shipments"
[0m15:57:12.449553 [debug] [Thread-4  ]: On model.dbt_data2bots.late_shp: BEGIN
[0m15:57:12.449899 [debug] [Thread-3  ]: On model.dbt_data2bots.best_performing_product: BEGIN
[0m15:57:12.450244 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:57:12.450614 [debug] [Thread-2  ]: On model.dbt_data2bots.agg_shipments: BEGIN
[0m15:57:12.450993 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m15:57:12.451360 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m15:57:12.451841 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m15:57:14.517249 [debug] [Thread-2  ]: SQL status: BEGIN in 2.0 seconds
[0m15:57:14.518403 [debug] [Thread-4  ]: SQL status: BEGIN in 2.0 seconds
[0m15:57:14.519334 [debug] [Thread-1  ]: SQL status: BEGIN in 2.0 seconds
[0m15:57:14.520307 [debug] [Thread-2  ]: Using postgres connection "model.dbt_data2bots.agg_shipments"
[0m15:57:14.521039 [debug] [Thread-4  ]: Using postgres connection "model.dbt_data2bots.late_shp"
[0m15:57:14.521746 [debug] [Thread-1  ]: Using postgres connection "model.dbt_data2bots.agg_public_holiday"
[0m15:57:14.522679 [debug] [Thread-2  ]: On model.dbt_data2bots.agg_shipments: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.agg_shipments"} */

  
    

  create  table "d2b_accessment"."abduafol4283_analytics"."agg_shipments__dbt_tmp"
  
  
    as
  
  (
    with base_shipment as (
    select
        shipment_id,
        order_id,
        date(shipment_date) as shipment_date,
        -- convert to a date column from text 
        date(delivery_date) as delivery_date -- convert to a date column from text 
    from
        abduafol4283_staging.shipment_deliveries
),
base_order as (
    select
        order_id,
        date(order_date) as order_date -- convert to a date column from text 
    from
        abduafol4283_staging.orders
),
order_shipments as (
    select
        base_shipment.shipment_id,
        base_shipment.order_id,
        base_shipment.shipment_date,
        base_shipment.delivery_date,
        base_order.order_date,
        (date(base_order.order_date) + INTERVAL '6 days') :: date as delivery_due_date,
        -- create a column for the delivery due date as 6 days after order_date
        (date(base_order.order_date) + INTERVAL '15 days') :: date as shipment_due_date,
        -- create a column for the shipment due date as 15 days after order_date
        '2022-09-01' :: date as "current-date" -- create a column for current date as specified
    from
        base_shipment
        left join base_order on base_shipment.order_id = base_order.order_id
),
late_shipment as (
    select
        current_date as ingestion_date,
        count(1) as tt_late_shipments
    from
        order_shipments
    where
        shipment_date >= delivery_due_date
        and delivery_date is NULL
),
undelivered_shipment as (
    select
        current_date as ingestion_date,
        count(1) as tt_undelivered_shipments
    from
        order_shipments
    where
        "current-date" > delivery_due_date
        and delivery_date is NULL
        and shipment_date is NULL
)
select
    late_shipment.ingestion_date,
    late_shipment.tt_late_shipments,
    undelivered_shipment.tt_undelivered_shipments
from
    late_shipment
    left join undelivered_shipment on late_shipment.ingestion_date = undelivered_shipment.ingestion_date
  );
  
[0m15:57:14.523723 [debug] [Thread-4  ]: On model.dbt_data2bots.late_shp: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.late_shp"} */

  
    

  create  table "d2b_accessment"."abduafol4283_analytics"."late_shp__dbt_tmp"
  
  
    as
  
  (
    with base_shipment as (
 select shipment_id,
	order_id,
	date(shipment_date) as shipment_date,  -- convert to a date column from text 
	date(delivery_date) as delivery_date  -- convert to a date column from text 
	
from abduafol4283_staging.shipment_deliveries
),

base_order as (
	select 	order_id,
			date(order_date) as order_date  -- convert to a date column from text 
	from abduafol4283_staging.orders
),

order_shipments as (
	select base_shipment.shipment_id,
			base_shipment.order_id,
	base_shipment.shipment_date,
	base_shipment.delivery_date,
	base_order.order_date,
	(date(base_order.order_date) + INTERVAL '6 days')::date as delivery_due_date   -- create a column for the delivery due date as 6 days after order_date
	
	from base_shipment 
	left join base_order
	on base_shipment.order_id = base_order.order_id
 
),
late_shipment as (
select *
from order_shipments
where shipment_date >= delivery_due_date)

select count(1)   --538
from late_shipment
  );
  
[0m15:57:14.524726 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.agg_public_holiday"} */

  
    

  create  table "d2b_accessment"."abduafol4283_analytics"."agg_public_holiday__dbt_tmp"
  
  
    as
  
  (
    with base_order as (
	select 	order_id,
			customer_id,
			date(order_date) as order_date  -- convert from text to date	
	from abduafol4283_staging.orders
),

base_date as (
	select 	calendar_dt,
			day_of_the_week_num,
			month_of_the_year_num,
			working_day
	from if_common.dim_dates
),
	
orders_date as (
select *
from base_order
left join base_date
on base_order.order_date = base_date.calendar_dt
where base_date.day_of_the_week_num  between 1 and 5
and working_day = false 
),

orders_count_agg as (
	select 	month_of_the_year_num month_,
			count(1) num_hol_orders
	from orders_date
	group by month_of_the_year_num
)
	
select 
		current_date as ingestion_date,
		max(case when month_ = 1 then num_hol_orders else 0 end )as tt_order_hol_jan,
		max(case when month_ = 2 then num_hol_orders else 0 end) as tt_order_hol_feb,
		max(case when month_ = 3 then num_hol_orders else 0 end) as tt_order_hol_mar,
		max(case when month_ = 4 then num_hol_orders else 0 end) as tt_order_hol_apr,
		max(case when month_ = 5 then num_hol_orders else 0 end) as tt_order_hol_may,
		max(case when month_ = 6 then num_hol_orders else 0 end) as tt_order_hol_jun,
		max(case when month_ = 7 then num_hol_orders else 0 end) as tt_order_hol_jul,
		max(case when month_ = 8 then num_hol_orders else 0 end) as tt_order_hol_aug,
		max(case when month_ = 9 then num_hol_orders else 0 end) as tt_order_hol_sep,
		max(case when month_ = 10 then num_hol_orders else 0 end) as tt_order_hol_oct,
		max(case when month_ = 11 then num_hol_orders else 0 end) as tt_order_hol_nov,
		max(case when month_ = 12 then num_hol_orders else 0 end) as tt_order_hol_dec
from orders_count_agg
  );
  
[0m15:57:14.766372 [debug] [Thread-1  ]: SQL status: SELECT 1 in 0.0 seconds
[0m15:57:14.766903 [debug] [Thread-4  ]: SQL status: SELECT 1 in 0.0 seconds
[0m15:57:14.776752 [debug] [Thread-1  ]: Using postgres connection "model.dbt_data2bots.agg_public_holiday"
[0m15:57:14.781951 [debug] [Thread-4  ]: Using postgres connection "model.dbt_data2bots.late_shp"
[0m15:57:14.782639 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.agg_public_holiday"} */
alter table "d2b_accessment"."abduafol4283_analytics"."agg_public_holiday" rename to "agg_public_holiday__dbt_backup"
[0m15:57:14.783181 [debug] [Thread-4  ]: On model.dbt_data2bots.late_shp: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.late_shp"} */
alter table "d2b_accessment"."abduafol4283_analytics"."late_shp" rename to "late_shp__dbt_backup"
[0m15:57:14.862809 [debug] [Thread-2  ]: SQL status: SELECT 1 in 0.0 seconds
[0m15:57:14.867282 [debug] [Thread-2  ]: Using postgres connection "model.dbt_data2bots.agg_shipments"
[0m15:57:14.867864 [debug] [Thread-2  ]: On model.dbt_data2bots.agg_shipments: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.agg_shipments"} */
alter table "d2b_accessment"."abduafol4283_analytics"."agg_shipments__dbt_tmp" rename to "agg_shipments"
[0m15:57:14.991241 [debug] [Thread-4  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m15:57:14.992145 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m15:57:14.998336 [debug] [Thread-4  ]: Using postgres connection "model.dbt_data2bots.late_shp"
[0m15:57:15.002829 [debug] [Thread-1  ]: Using postgres connection "model.dbt_data2bots.agg_public_holiday"
[0m15:57:15.003396 [debug] [Thread-4  ]: On model.dbt_data2bots.late_shp: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.late_shp"} */
alter table "d2b_accessment"."abduafol4283_analytics"."late_shp__dbt_tmp" rename to "late_shp"
[0m15:57:15.003903 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.agg_public_holiday"} */
alter table "d2b_accessment"."abduafol4283_analytics"."agg_public_holiday__dbt_tmp" rename to "agg_public_holiday"
[0m15:57:15.013050 [debug] [Thread-2  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m15:57:15.036400 [debug] [Thread-2  ]: On model.dbt_data2bots.agg_shipments: COMMIT
[0m15:57:15.036987 [debug] [Thread-2  ]: Using postgres connection "model.dbt_data2bots.agg_shipments"
[0m15:57:15.037372 [debug] [Thread-2  ]: On model.dbt_data2bots.agg_shipments: COMMIT
[0m15:57:15.159813 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m15:57:15.160694 [debug] [Thread-4  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m15:57:15.161326 [debug] [Thread-3  ]: SQL status: BEGIN in 3.0 seconds
[0m15:57:15.168872 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: COMMIT
[0m15:57:15.171792 [debug] [Thread-4  ]: On model.dbt_data2bots.late_shp: COMMIT
[0m15:57:15.172278 [debug] [Thread-3  ]: Using postgres connection "model.dbt_data2bots.best_performing_product"
[0m15:57:15.172717 [debug] [Thread-1  ]: Using postgres connection "model.dbt_data2bots.agg_public_holiday"
[0m15:57:15.173139 [debug] [Thread-4  ]: Using postgres connection "model.dbt_data2bots.late_shp"
[0m15:57:15.173945 [debug] [Thread-3  ]: On model.dbt_data2bots.best_performing_product: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.best_performing_product"} */

  
    

  create  table "d2b_accessment"."abduafol4283_analytics"."best_performing_product__dbt_tmp"
  
  
    as
  
  (
    with base_order as (
    select
        order_id,
        product_id,
        date(order_date) as order_date -- convert to a date column from text 
    from
        abduafol4283_staging.orders
),
rank_orders as (
    select
        product_id,
        order_date as most_ordered_day,
        count(product_id) as num_of_orders,
        row_number() over (
            partition by product_id
            order by
                count(product_id) desc
        ) rank
    from
        base_order
    group by
        order_date,
        product_id
),
agg_order as (
    select
        *
    from
        rank_orders
    where
        rank = 1
),
base_shipment as (
    select
        shipment_id,
        order_id,
        date(shipment_date) as shipment_date,
        -- convert to a date column from text 
        date(delivery_date) as delivery_date -- convert to a date column from text 
    from
        abduafol4283_staging.shipment_deliveries
),
order_shipments as (
    select
        base_shipment.shipment_id,
        base_shipment.order_id,
        base_order.product_id,
        base_shipment.shipment_date,
        base_shipment.delivery_date,
        base_order.order_date,
        (date(base_order.order_date) + interval '6 days') :: date as delivery_due_date,
        -- create a column for the delivery due date as 6 days after order_date
        (date(base_order.order_date) + interval '15 days') :: date as shipment_due_date,
        -- create a column for the shipment due date as 15 days after order_date
        '2022-09-01' :: date as "current-date" -- create a column for current date
    from
        base_shipment
        left join base_order on base_shipment.order_id = base_order.order_id
),
late_shipment as (
    select
        current_date as ingestion_date,
        order_date,
        product_id,
        count(product_id) count_late_shipment --count() as tt_late_shipments
    from
        order_shipments
    where
        shipment_date >= delivery_due_date
        and delivery_date is null
    group by
        order_date,
        product_id
),
undelivered_shipment as (
    select
        current_date as ingestion_date,
        order_date,
        product_id,
        count(product_id) count_undelivered_shipment --count(1) as tt_undelivered_shipments
    from
        order_shipments
    where
        "current-date" > delivery_due_date
        and delivery_date is null
        and shipment_date is null
    group by
        order_date,
        product_id
),
base_date as (
    select
        calendar_dt,
        day_of_the_week_num,
        month_of_the_year_num,
        working_day,
        case
            when day_of_the_week_num between 1
            and 5
            and working_day = false then true
            else false
        end as is_public_holiday
    from
        if_common.dim_dates
),
base_product as (
    select
        product_id,
        product_name
    from
        if_common.dim_products
),
product_orders_date as (
    select
        agg_order.product_id,
        base_product.product_name,
        agg_order.num_of_orders,
        agg_order.most_ordered_day,
        base_date.is_public_holiday
    from
        agg_order
        left join base_date on agg_order.most_ordered_day = base_date.calendar_dt
        left join base_product on agg_order.product_id = base_product.product_id
),
agg_review as (
    select
        product_id,
        sum(
            case
                when review = 1 then 1
                else 0
            end
        ) as star_1_count,
        sum(
            case
                when review = 2 then 1
                else 0
            end
        ) as star_2_count,
        sum(
            case
                when review = 3 then 1
                else 0
            end
        ) as star_3_count,
        sum(
            case
                when review = 4 then 1
                else 0
            end
        ) as star_4_count,
        sum(
            case
                when review = 5 then 1
                else 0
            end
        ) as star_5_count,
        count(*) as total_reviews
    from
        abduafol4283_staging.reviews
    group by
        product_id
),
highest_review_pct as (
    select
        product_id,
        total_reviews,
        round(star_1_count / total_reviews :: numeric * 100, 2) as pct_one_star_review,
        round(star_2_count / total_reviews :: numeric * 100, 2) as pct_two_star_review,
        round(star_3_count / total_reviews :: numeric * 100, 2) as pct_three_star_review,
        round(star_4_count / total_reviews :: numeric * 100, 2) as pct_four_star_review,
        round(star_5_count / total_reviews :: numeric * 100, 2) as pct_five_star_review
    from
        agg_review
    order by
        total_reviews desc
    limit
        1
), final as (
    select
        current_date as ingestion_date,
        product_orders_date.product_name,
        product_orders_date.most_ordered_day,
        product_orders_date.is_public_holiday,
        highest_review_pct.total_reviews as tt_review_points,
        highest_review_pct.pct_one_star_review,
        highest_review_pct.pct_two_star_review,
        highest_review_pct.pct_three_star_review,
        highest_review_pct.pct_four_star_review,
        highest_review_pct.pct_five_star_review,
        round(
            case
                when count_late_shipment is not null then count_late_shipment
                else 0
            end / product_orders_date.num_of_orders :: numeric * 100,
            2
        ) as pct_early_shipments,
        round(
            case
                when count_undelivered_shipment is not null then count_undelivered_shipment
                else 0
            end / product_orders_date.num_of_orders :: numeric * 100,
            2
        ) as pct_early_shipments
    from
        highest_review_pct
        left join product_orders_date on highest_review_pct.product_id = product_orders_date.product_id
        left join late_shipment on highest_review_pct.product_id = late_shipment.product_id
        and product_orders_date.most_ordered_day = late_shipment.order_date
        left join undelivered_shipment on highest_review_pct.product_id = undelivered_shipment.product_id
        and product_orders_date.most_ordered_day = undelivered_shipment.order_date
)
select
    *
from
    final
  );
  
[0m15:57:15.174750 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: COMMIT
[0m15:57:15.175145 [debug] [Thread-4  ]: On model.dbt_data2bots.late_shp: COMMIT
[0m15:57:15.204940 [debug] [Thread-2  ]: SQL status: COMMIT in 0.0 seconds
[0m15:57:15.215354 [debug] [Thread-2  ]: Using postgres connection "model.dbt_data2bots.agg_shipments"
[0m15:57:15.215887 [debug] [Thread-2  ]: On model.dbt_data2bots.agg_shipments: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.agg_shipments"} */
drop table if exists "d2b_accessment"."abduafol4283_analytics"."agg_shipments__dbt_backup" cascade
[0m15:57:15.412136 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
[0m15:57:15.414276 [debug] [Thread-4  ]: SQL status: COMMIT in 0.0 seconds
[0m15:57:15.415294 [debug] [Thread-2  ]: SQL status: DROP TABLE in 0.0 seconds
[0m15:57:15.416473 [debug] [Thread-3  ]: Postgres adapter: Postgres error: column "pct_early_shipments" specified more than once

[0m15:57:15.423432 [debug] [Thread-1  ]: Using postgres connection "model.dbt_data2bots.agg_public_holiday"
[0m15:57:15.428544 [debug] [Thread-4  ]: Using postgres connection "model.dbt_data2bots.late_shp"
[0m15:57:15.431131 [debug] [Thread-2  ]: Timing info for model.dbt_data2bots.agg_shipments (execute): 15:57:12.396012 => 15:57:15.430849
[0m15:57:15.431770 [debug] [Thread-3  ]: On model.dbt_data2bots.best_performing_product: ROLLBACK
[0m15:57:15.432343 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.agg_public_holiday"} */
drop table if exists "d2b_accessment"."abduafol4283_analytics"."agg_public_holiday__dbt_backup" cascade
[0m15:57:15.432894 [debug] [Thread-4  ]: On model.dbt_data2bots.late_shp: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.late_shp"} */
drop table if exists "d2b_accessment"."abduafol4283_analytics"."late_shp__dbt_backup" cascade
[0m15:57:15.433460 [debug] [Thread-2  ]: On model.dbt_data2bots.agg_shipments: Close
[0m15:57:15.435352 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32ba5cbc-debe-4d98-87b4-8e0ba2cee898', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bf133c610>]}
[0m15:57:15.436784 [info ] [Thread-2  ]: 2 of 4 OK created sql table model abduafol4283_analytics.agg_shipments ......... [[32mSELECT 1[0m in 3.06s]
[0m15:57:15.437705 [debug] [Thread-2  ]: Finished running node model.dbt_data2bots.agg_shipments
[0m15:57:15.628716 [debug] [Thread-4  ]: SQL status: DROP TABLE in 0.0 seconds
[0m15:57:15.629259 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
[0m15:57:15.629938 [debug] [Thread-3  ]: Timing info for model.dbt_data2bots.best_performing_product (execute): 15:57:12.442869 => 15:57:15.629727
[0m15:57:15.631579 [debug] [Thread-4  ]: Timing info for model.dbt_data2bots.late_shp (execute): 15:57:12.434487 => 15:57:15.631386
[0m15:57:15.632937 [debug] [Thread-1  ]: Timing info for model.dbt_data2bots.agg_public_holiday (execute): 15:57:12.438945 => 15:57:15.632759
[0m15:57:15.633463 [debug] [Thread-3  ]: On model.dbt_data2bots.best_performing_product: Close
[0m15:57:15.633926 [debug] [Thread-4  ]: On model.dbt_data2bots.late_shp: Close
[0m15:57:15.634331 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: Close
[0m15:57:15.636096 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32ba5cbc-debe-4d98-87b4-8e0ba2cee898', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bd0838fa0>]}
[0m15:57:15.636814 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32ba5cbc-debe-4d98-87b4-8e0ba2cee898', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bb00d43d0>]}
[0m15:57:15.637450 [info ] [Thread-1  ]: 1 of 4 OK created sql table model abduafol4283_analytics.agg_public_holiday .... [[32mSELECT 1[0m in 3.26s]
[0m15:57:15.638055 [info ] [Thread-4  ]: 4 of 4 OK created sql table model abduafol4283_analytics.late_shp .............. [[32mSELECT 1[0m in 3.26s]
[0m15:57:15.638715 [debug] [Thread-1  ]: Finished running node model.dbt_data2bots.agg_public_holiday
[0m15:57:15.639399 [debug] [Thread-4  ]: Finished running node model.dbt_data2bots.late_shp
[0m15:57:15.643680 [debug] [Thread-3  ]: Database Error in model best_performing_product (models/abduafol4283_analytics/best_performing_product.sql)
  column "pct_early_shipments" specified more than once
  compiled Code at target/run/dbt_data2bots/models/abduafol4283_analytics/best_performing_product.sql
[0m15:57:15.644243 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '32ba5cbc-debe-4d98-87b4-8e0ba2cee898', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bd0854760>]}
[0m15:57:15.644814 [error] [Thread-3  ]: 3 of 4 ERROR creating sql table model abduafol4283_analytics.best_performing_product  [[31mERROR[0m in 3.27s]
[0m15:57:15.645394 [debug] [Thread-3  ]: Finished running node model.dbt_data2bots.best_performing_product
[0m15:57:15.647356 [debug] [MainThread]: Using postgres connection "master"
[0m15:57:15.647744 [debug] [MainThread]: On master: BEGIN
[0m15:57:15.648042 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:57:18.342384 [debug] [MainThread]: SQL status: BEGIN in 3.0 seconds
[0m15:57:18.343820 [debug] [MainThread]: On master: COMMIT
[0m15:57:18.344210 [debug] [MainThread]: Using postgres connection "master"
[0m15:57:18.344521 [debug] [MainThread]: On master: COMMIT
[0m15:57:18.546620 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m15:57:18.547172 [debug] [MainThread]: On master: Close
[0m15:57:18.548037 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:57:18.548384 [debug] [MainThread]: Connection 'model.dbt_data2bots.agg_public_holiday' was properly closed.
[0m15:57:18.548666 [debug] [MainThread]: Connection 'model.dbt_data2bots.agg_shipments' was properly closed.
[0m15:57:18.548929 [debug] [MainThread]: Connection 'model.dbt_data2bots.best_performing_product' was properly closed.
[0m15:57:18.549200 [debug] [MainThread]: Connection 'model.dbt_data2bots.late_shp' was properly closed.
[0m15:57:18.549559 [info ] [MainThread]: 
[0m15:57:18.550004 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 14.69 seconds (14.69s).
[0m15:57:18.551077 [debug] [MainThread]: Command end result
[0m15:57:18.559622 [info ] [MainThread]: 
[0m15:57:18.560103 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m15:57:18.560409 [info ] [MainThread]: 
[0m15:57:18.560717 [error] [MainThread]: [33mDatabase Error in model best_performing_product (models/abduafol4283_analytics/best_performing_product.sql)[0m
[0m15:57:18.561011 [error] [MainThread]:   column "pct_early_shipments" specified more than once
[0m15:57:18.561300 [error] [MainThread]:   compiled Code at target/run/dbt_data2bots/models/abduafol4283_analytics/best_performing_product.sql
[0m15:57:18.561586 [info ] [MainThread]: 
[0m15:57:18.561914 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=0 TOTAL=4
[0m15:57:18.562603 [debug] [MainThread]: Command `dbt run` failed at 15:57:18.562510 after 15.00 seconds
[0m15:57:18.563013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bf079ec70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bf1347c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8bf1345be0>]}
[0m15:57:18.563382 [debug] [MainThread]: Flushing usage events
[0m15:59:01.459122 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4789a6c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc47a37bd00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc47a3b0400>]}


============================== 15:59:01.465572 | f24c0769-5b9b-4b87-86cd-a1d45cca206b ==============================
[0m15:59:01.465572 [info ] [MainThread]: Running with dbt=1.6.0
[0m15:59:01.466129 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/abdulqadriafolabi/Documents/DR/Data2bots/dbt_data2bots/logs', 'debug': 'False', 'profiles_dir': '/Users/abdulqadriafolabi/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m15:59:01.598753 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f24c0769-5b9b-4b87-86cd-a1d45cca206b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc478174f40>]}
[0m15:59:01.607959 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f24c0769-5b9b-4b87-86cd-a1d45cca206b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc47a70ea90>]}
[0m15:59:01.608721 [info ] [MainThread]: Registered adapter: postgres=1.6.0
[0m15:59:01.628861 [debug] [MainThread]: checksum: 08dabea3fc65f9f8dd48613681c2bcd827397a7ba9ce4c6d52980aba1215e8c9, vars: {}, profile: , target: , version: 1.6.0
[0m15:59:01.660997 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 0 files added, 1 files changed.
[0m15:59:01.661556 [debug] [MainThread]: Partial parsing: deleted file: dbt_data2bots://models/abduafol4283_analytics/late_shp.sql
[0m15:59:01.662024 [debug] [MainThread]: Partial parsing: updated file: dbt_data2bots://models/abduafol4283_analytics/best_performing_product.sql
[0m15:59:01.700529 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f24c0769-5b9b-4b87-86cd-a1d45cca206b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4791d10d0>]}
[0m15:59:01.712276 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f24c0769-5b9b-4b87-86cd-a1d45cca206b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc438085880>]}
[0m15:59:01.712772 [info ] [MainThread]: Found 3 models, 0 sources, 0 exposures, 0 metrics, 349 macros, 0 groups, 0 semantic models
[0m15:59:01.713136 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f24c0769-5b9b-4b87-86cd-a1d45cca206b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc468422f70>]}
[0m15:59:01.714623 [info ] [MainThread]: 
[0m15:59:01.715312 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m15:59:01.716348 [debug] [ThreadPool]: Acquiring new postgres connection 'list_d2b_accessment'
[0m15:59:01.725992 [debug] [ThreadPool]: Using postgres connection "list_d2b_accessment"
[0m15:59:01.726536 [debug] [ThreadPool]: On list_d2b_accessment: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "connection_name": "list_d2b_accessment"} */

    select distinct nspname from pg_namespace
  
[0m15:59:01.726930 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:59:03.863542 [debug] [ThreadPool]: SQL status: SELECT 124 in 2.0 seconds
[0m15:59:03.867961 [debug] [ThreadPool]: On list_d2b_accessment: Close
[0m15:59:03.870152 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_d2b_accessment, now list_d2b_accessment_abduafol4283_analytics)
[0m15:59:03.876934 [debug] [ThreadPool]: Using postgres connection "list_d2b_accessment_abduafol4283_analytics"
[0m15:59:03.877498 [debug] [ThreadPool]: On list_d2b_accessment_abduafol4283_analytics: BEGIN
[0m15:59:03.877880 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:59:05.924469 [debug] [ThreadPool]: SQL status: BEGIN in 2.0 seconds
[0m15:59:05.926577 [debug] [ThreadPool]: Using postgres connection "list_d2b_accessment_abduafol4283_analytics"
[0m15:59:05.927725 [debug] [ThreadPool]: On list_d2b_accessment_abduafol4283_analytics: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "connection_name": "list_d2b_accessment_abduafol4283_analytics"} */
select
      'd2b_accessment' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'abduafol4283_analytics'
    union all
    select
      'd2b_accessment' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'abduafol4283_analytics'
    union all
    select
      'd2b_accessment' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'abduafol4283_analytics'
  
[0m15:59:06.152955 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.0 seconds
[0m15:59:06.159146 [debug] [ThreadPool]: On list_d2b_accessment_abduafol4283_analytics: ROLLBACK
[0m15:59:06.363858 [debug] [ThreadPool]: On list_d2b_accessment_abduafol4283_analytics: Close
[0m15:59:06.375282 [debug] [MainThread]: Using postgres connection "master"
[0m15:59:06.376005 [debug] [MainThread]: On master: BEGIN
[0m15:59:06.376458 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:59:10.061321 [debug] [MainThread]: SQL status: BEGIN in 4.0 seconds
[0m15:59:10.064861 [debug] [MainThread]: Using postgres connection "master"
[0m15:59:10.066066 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v', 'm')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m15:59:10.767114 [debug] [MainThread]: SQL status: SELECT 61 in 1.0 seconds
[0m15:59:10.775659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f24c0769-5b9b-4b87-86cd-a1d45cca206b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4791edeb0>]}
[0m15:59:10.777109 [debug] [MainThread]: On master: ROLLBACK
[0m15:59:10.994793 [debug] [MainThread]: Using postgres connection "master"
[0m15:59:10.996651 [debug] [MainThread]: On master: BEGIN
[0m15:59:11.424325 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
[0m15:59:11.424979 [debug] [MainThread]: On master: COMMIT
[0m15:59:11.425366 [debug] [MainThread]: Using postgres connection "master"
[0m15:59:11.425722 [debug] [MainThread]: On master: COMMIT
[0m15:59:11.629730 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m15:59:11.630885 [debug] [MainThread]: On master: Close
[0m15:59:11.632579 [info ] [MainThread]: Concurrency: 5 threads (target='prod')
[0m15:59:11.633205 [info ] [MainThread]: 
[0m15:59:11.639034 [debug] [Thread-1  ]: Began running node model.dbt_data2bots.agg_public_holiday
[0m15:59:11.639785 [debug] [Thread-2  ]: Began running node model.dbt_data2bots.agg_shipments
[0m15:59:11.640530 [debug] [Thread-3  ]: Began running node model.dbt_data2bots.best_performing_product
[0m15:59:11.641328 [info ] [Thread-1  ]: 1 of 3 START sql table model abduafol4283_analytics.agg_public_holiday ......... [RUN]
[0m15:59:11.642135 [info ] [Thread-2  ]: 2 of 3 START sql table model abduafol4283_analytics.agg_shipments .............. [RUN]
[0m15:59:11.642812 [info ] [Thread-3  ]: 3 of 3 START sql table model abduafol4283_analytics.best_performing_product .... [RUN]
[0m15:59:11.643805 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_d2b_accessment_abduafol4283_analytics, now model.dbt_data2bots.agg_public_holiday)
[0m15:59:11.644719 [debug] [Thread-2  ]: Acquiring new postgres connection 'model.dbt_data2bots.agg_shipments'
[0m15:59:11.645559 [debug] [Thread-3  ]: Acquiring new postgres connection 'model.dbt_data2bots.best_performing_product'
[0m15:59:11.646060 [debug] [Thread-1  ]: Began compiling node model.dbt_data2bots.agg_public_holiday
[0m15:59:11.646512 [debug] [Thread-2  ]: Began compiling node model.dbt_data2bots.agg_shipments
[0m15:59:11.646957 [debug] [Thread-3  ]: Began compiling node model.dbt_data2bots.best_performing_product
[0m15:59:11.655120 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_data2bots.agg_public_holiday"
[0m15:59:11.657401 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_data2bots.agg_shipments"
[0m15:59:11.659562 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbt_data2bots.best_performing_product"
[0m15:59:11.660855 [debug] [Thread-1  ]: Timing info for model.dbt_data2bots.agg_public_holiday (compile): 15:59:11.647305 => 15:59:11.660576
[0m15:59:11.661344 [debug] [Thread-1  ]: Began executing node model.dbt_data2bots.agg_public_holiday
[0m15:59:11.667877 [debug] [Thread-2  ]: Timing info for model.dbt_data2bots.agg_shipments (compile): 15:59:11.655582 => 15:59:11.667520
[0m15:59:11.668427 [debug] [Thread-3  ]: Timing info for model.dbt_data2bots.best_performing_product (compile): 15:59:11.657835 => 15:59:11.668200
[0m15:59:11.698832 [debug] [Thread-2  ]: Began executing node model.dbt_data2bots.agg_shipments
[0m15:59:11.701694 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbt_data2bots.agg_public_holiday"
[0m15:59:11.702136 [debug] [Thread-3  ]: Began executing node model.dbt_data2bots.best_performing_product
[0m15:59:11.706528 [debug] [Thread-2  ]: Writing runtime sql for node "model.dbt_data2bots.agg_shipments"
[0m15:59:11.711062 [debug] [Thread-3  ]: Writing runtime sql for node "model.dbt_data2bots.best_performing_product"
[0m15:59:11.711690 [debug] [Thread-1  ]: Using postgres connection "model.dbt_data2bots.agg_public_holiday"
[0m15:59:11.712198 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: BEGIN
[0m15:59:11.712592 [debug] [Thread-2  ]: Using postgres connection "model.dbt_data2bots.agg_shipments"
[0m15:59:11.716384 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:59:11.716799 [debug] [Thread-3  ]: Using postgres connection "model.dbt_data2bots.best_performing_product"
[0m15:59:11.717211 [debug] [Thread-2  ]: On model.dbt_data2bots.agg_shipments: BEGIN
[0m15:59:11.717745 [debug] [Thread-3  ]: On model.dbt_data2bots.best_performing_product: BEGIN
[0m15:59:11.718117 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m15:59:11.718464 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m15:59:14.507664 [debug] [Thread-1  ]: SQL status: BEGIN in 3.0 seconds
[0m15:59:14.511016 [debug] [Thread-3  ]: SQL status: BEGIN in 3.0 seconds
[0m15:59:14.512078 [debug] [Thread-2  ]: SQL status: BEGIN in 3.0 seconds
[0m15:59:14.512941 [debug] [Thread-1  ]: Using postgres connection "model.dbt_data2bots.agg_public_holiday"
[0m15:59:14.513937 [debug] [Thread-3  ]: Using postgres connection "model.dbt_data2bots.best_performing_product"
[0m15:59:14.514797 [debug] [Thread-2  ]: Using postgres connection "model.dbt_data2bots.agg_shipments"
[0m15:59:14.515914 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.agg_public_holiday"} */

  
    

  create  table "d2b_accessment"."abduafol4283_analytics"."agg_public_holiday__dbt_tmp"
  
  
    as
  
  (
    with base_order as (
	select 	order_id,
			customer_id,
			date(order_date) as order_date  -- convert from text to date	
	from abduafol4283_staging.orders
),

base_date as (
	select 	calendar_dt,
			day_of_the_week_num,
			month_of_the_year_num,
			working_day
	from if_common.dim_dates
),
	
orders_date as (
select *
from base_order
left join base_date
on base_order.order_date = base_date.calendar_dt
where base_date.day_of_the_week_num  between 1 and 5
and working_day = false 
),

orders_count_agg as (
	select 	month_of_the_year_num month_,
			count(1) num_hol_orders
	from orders_date
	group by month_of_the_year_num
)
	
select 
		current_date as ingestion_date,
		max(case when month_ = 1 then num_hol_orders else 0 end )as tt_order_hol_jan,
		max(case when month_ = 2 then num_hol_orders else 0 end) as tt_order_hol_feb,
		max(case when month_ = 3 then num_hol_orders else 0 end) as tt_order_hol_mar,
		max(case when month_ = 4 then num_hol_orders else 0 end) as tt_order_hol_apr,
		max(case when month_ = 5 then num_hol_orders else 0 end) as tt_order_hol_may,
		max(case when month_ = 6 then num_hol_orders else 0 end) as tt_order_hol_jun,
		max(case when month_ = 7 then num_hol_orders else 0 end) as tt_order_hol_jul,
		max(case when month_ = 8 then num_hol_orders else 0 end) as tt_order_hol_aug,
		max(case when month_ = 9 then num_hol_orders else 0 end) as tt_order_hol_sep,
		max(case when month_ = 10 then num_hol_orders else 0 end) as tt_order_hol_oct,
		max(case when month_ = 11 then num_hol_orders else 0 end) as tt_order_hol_nov,
		max(case when month_ = 12 then num_hol_orders else 0 end) as tt_order_hol_dec
from orders_count_agg
  );
  
[0m15:59:14.517507 [debug] [Thread-3  ]: On model.dbt_data2bots.best_performing_product: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.best_performing_product"} */

  
    

  create  table "d2b_accessment"."abduafol4283_analytics"."best_performing_product__dbt_tmp"
  
  
    as
  
  (
    with base_order as (
    select
        order_id,
        product_id,
        date(order_date) as order_date -- convert to a date column from text 
    from
        abduafol4283_staging.orders
),
rank_orders as (
    select
        product_id,
        order_date as most_ordered_day,
        count(product_id) as num_of_orders,
        row_number() over (
            partition by product_id
            order by
                count(product_id) desc
        ) rank
    from
        base_order
    group by
        order_date,
        product_id
),
agg_order as (
    select
        *
    from
        rank_orders
    where
        rank = 1
),
base_shipment as (
    select
        shipment_id,
        order_id,
        date(shipment_date) as shipment_date,
        -- convert to a date column from text 
        date(delivery_date) as delivery_date -- convert to a date column from text 
    from
        abduafol4283_staging.shipment_deliveries
),
order_shipments as (
    select
        base_shipment.shipment_id,
        base_shipment.order_id,
        base_order.product_id,
        base_shipment.shipment_date,
        base_shipment.delivery_date,
        base_order.order_date,
        (date(base_order.order_date) + interval '6 days') :: date as delivery_due_date,
        -- create a column for the delivery due date as 6 days after order_date
        (date(base_order.order_date) + interval '15 days') :: date as shipment_due_date,
        -- create a column for the shipment due date as 15 days after order_date
        '2022-09-01' :: date as "current-date" -- create a column for current date
    from
        base_shipment
        left join base_order on base_shipment.order_id = base_order.order_id
),
late_shipment as (
    select
        current_date as ingestion_date,
        order_date,
        product_id,
        count(product_id) count_late_shipment --count() as tt_late_shipments
    from
        order_shipments
    where
        shipment_date >= delivery_due_date
        and delivery_date is null
    group by
        order_date,
        product_id
),
undelivered_shipment as (
    select
        current_date as ingestion_date,
        order_date,
        product_id,
        count(product_id) count_undelivered_shipment --count(1) as tt_undelivered_shipments
    from
        order_shipments
    where
        "current-date" > delivery_due_date
        and delivery_date is null
        and shipment_date is null
    group by
        order_date,
        product_id
),
base_date as (
    select
        calendar_dt,
        day_of_the_week_num,
        month_of_the_year_num,
        working_day,
        case
            when day_of_the_week_num between 1
            and 5
            and working_day = false then true
            else false
        end as is_public_holiday
    from
        if_common.dim_dates
),
base_product as (
    select
        product_id,
        product_name
    from
        if_common.dim_products
),
product_orders_date as (
    select
        agg_order.product_id,
        base_product.product_name,
        agg_order.num_of_orders,
        agg_order.most_ordered_day,
        base_date.is_public_holiday
    from
        agg_order
        left join base_date on agg_order.most_ordered_day = base_date.calendar_dt
        left join base_product on agg_order.product_id = base_product.product_id
),
agg_review as (
    select
        product_id,
        sum(
            case
                when review = 1 then 1
                else 0
            end
        ) as star_1_count,
        sum(
            case
                when review = 2 then 1
                else 0
            end
        ) as star_2_count,
        sum(
            case
                when review = 3 then 1
                else 0
            end
        ) as star_3_count,
        sum(
            case
                when review = 4 then 1
                else 0
            end
        ) as star_4_count,
        sum(
            case
                when review = 5 then 1
                else 0
            end
        ) as star_5_count,
        count(*) as total_reviews
    from
        abduafol4283_staging.reviews
    group by
        product_id
),
highest_review_pct as (
    select
        product_id,
        total_reviews,
        round(star_1_count / total_reviews :: numeric * 100, 2) as pct_one_star_review,
        round(star_2_count / total_reviews :: numeric * 100, 2) as pct_two_star_review,
        round(star_3_count / total_reviews :: numeric * 100, 2) as pct_three_star_review,
        round(star_4_count / total_reviews :: numeric * 100, 2) as pct_four_star_review,
        round(star_5_count / total_reviews :: numeric * 100, 2) as pct_five_star_review
    from
        agg_review
    order by
        total_reviews desc
    limit
        1
), final as (
    select
        current_date as ingestion_date,
        product_orders_date.product_name,
        product_orders_date.most_ordered_day,
        product_orders_date.is_public_holiday,
        highest_review_pct.total_reviews as tt_review_points,
        highest_review_pct.pct_one_star_review,
        highest_review_pct.pct_two_star_review,
        highest_review_pct.pct_three_star_review,
        highest_review_pct.pct_four_star_review,
        highest_review_pct.pct_five_star_review,
        round(
            case
                when count_late_shipment is not null then count_late_shipment
                else 0
            end / product_orders_date.num_of_orders :: numeric * 100,
            2
        ) as pct_early_shipments,
        round(
            case
                when count_undelivered_shipment is not null then count_undelivered_shipment
                else 0
            end / product_orders_date.num_of_orders :: numeric * 100,
            2
        ) as pct_late_shipments
    from
        highest_review_pct
        left join product_orders_date on highest_review_pct.product_id = product_orders_date.product_id
        left join late_shipment on highest_review_pct.product_id = late_shipment.product_id
        and product_orders_date.most_ordered_day = late_shipment.order_date
        left join undelivered_shipment on highest_review_pct.product_id = undelivered_shipment.product_id
        and product_orders_date.most_ordered_day = undelivered_shipment.order_date
)
select
    *
from
    final
  );
  
[0m15:59:14.519022 [debug] [Thread-2  ]: On model.dbt_data2bots.agg_shipments: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.agg_shipments"} */

  
    

  create  table "d2b_accessment"."abduafol4283_analytics"."agg_shipments__dbt_tmp"
  
  
    as
  
  (
    with base_shipment as (
    select
        shipment_id,
        order_id,
        date(shipment_date) as shipment_date,
        -- convert to a date column from text 
        date(delivery_date) as delivery_date -- convert to a date column from text 
    from
        abduafol4283_staging.shipment_deliveries
),
base_order as (
    select
        order_id,
        date(order_date) as order_date -- convert to a date column from text 
    from
        abduafol4283_staging.orders
),
order_shipments as (
    select
        base_shipment.shipment_id,
        base_shipment.order_id,
        base_shipment.shipment_date,
        base_shipment.delivery_date,
        base_order.order_date,
        (date(base_order.order_date) + INTERVAL '6 days') :: date as delivery_due_date,
        -- create a column for the delivery due date as 6 days after order_date
        (date(base_order.order_date) + INTERVAL '15 days') :: date as shipment_due_date,
        -- create a column for the shipment due date as 15 days after order_date
        '2022-09-01' :: date as "current-date" -- create a column for current date as specified
    from
        base_shipment
        left join base_order on base_shipment.order_id = base_order.order_id
),
late_shipment as (
    select
        current_date as ingestion_date,
        count(1) as tt_late_shipments
    from
        order_shipments
    where
        shipment_date >= delivery_due_date
        and delivery_date is NULL
),
undelivered_shipment as (
    select
        current_date as ingestion_date,
        count(1) as tt_undelivered_shipments
    from
        order_shipments
    where
        "current-date" > delivery_due_date
        and delivery_date is NULL
        and shipment_date is NULL
)
select
    late_shipment.ingestion_date,
    late_shipment.tt_late_shipments,
    undelivered_shipment.tt_undelivered_shipments
from
    late_shipment
    left join undelivered_shipment on late_shipment.ingestion_date = undelivered_shipment.ingestion_date
  );
  
[0m15:59:14.759905 [debug] [Thread-1  ]: SQL status: SELECT 1 in 0.0 seconds
[0m15:59:14.774854 [debug] [Thread-1  ]: Using postgres connection "model.dbt_data2bots.agg_public_holiday"
[0m15:59:14.775699 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.agg_public_holiday"} */
alter table "d2b_accessment"."abduafol4283_analytics"."agg_public_holiday" rename to "agg_public_holiday__dbt_backup"
[0m15:59:14.800355 [debug] [Thread-2  ]: SQL status: SELECT 1 in 0.0 seconds
[0m15:59:14.805719 [debug] [Thread-2  ]: Using postgres connection "model.dbt_data2bots.agg_shipments"
[0m15:59:14.806698 [debug] [Thread-2  ]: On model.dbt_data2bots.agg_shipments: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.agg_shipments"} */
alter table "d2b_accessment"."abduafol4283_analytics"."agg_shipments" rename to "agg_shipments__dbt_backup"
[0m15:59:15.036808 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m15:59:15.047023 [debug] [Thread-1  ]: Using postgres connection "model.dbt_data2bots.agg_public_holiday"
[0m15:59:15.047863 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.agg_public_holiday"} */
alter table "d2b_accessment"."abduafol4283_analytics"."agg_public_holiday__dbt_tmp" rename to "agg_public_holiday"
[0m15:59:15.294704 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m15:59:15.322434 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: COMMIT
[0m15:59:15.323069 [debug] [Thread-1  ]: Using postgres connection "model.dbt_data2bots.agg_public_holiday"
[0m15:59:15.323499 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: COMMIT
[0m15:59:15.566893 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
[0m15:59:15.588515 [debug] [Thread-1  ]: Using postgres connection "model.dbt_data2bots.agg_public_holiday"
[0m15:59:15.589576 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.agg_public_holiday"} */
drop table if exists "d2b_accessment"."abduafol4283_analytics"."agg_public_holiday__dbt_backup" cascade
[0m15:59:15.663770 [debug] [Thread-3  ]: SQL status: SELECT 1 in 1.0 seconds
[0m15:59:15.669553 [debug] [Thread-3  ]: Using postgres connection "model.dbt_data2bots.best_performing_product"
[0m15:59:15.670296 [debug] [Thread-3  ]: On model.dbt_data2bots.best_performing_product: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.best_performing_product"} */
alter table "d2b_accessment"."abduafol4283_analytics"."best_performing_product__dbt_tmp" rename to "best_performing_product"
[0m15:59:15.828203 [debug] [Thread-2  ]: SQL status: ALTER TABLE in 1.0 seconds
[0m15:59:15.830056 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
[0m15:59:15.834127 [debug] [Thread-2  ]: Using postgres connection "model.dbt_data2bots.agg_shipments"
[0m15:59:15.835632 [debug] [Thread-1  ]: Timing info for model.dbt_data2bots.agg_public_holiday (execute): 15:59:11.661653 => 15:59:15.835432
[0m15:59:15.836046 [debug] [Thread-2  ]: On model.dbt_data2bots.agg_shipments: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.agg_shipments"} */
alter table "d2b_accessment"."abduafol4283_analytics"."agg_shipments__dbt_tmp" rename to "agg_shipments"
[0m15:59:15.836459 [debug] [Thread-1  ]: On model.dbt_data2bots.agg_public_holiday: Close
[0m15:59:15.837439 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f24c0769-5b9b-4b87-86cd-a1d45cca206b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4791ffca0>]}
[0m15:59:15.838566 [info ] [Thread-1  ]: 1 of 3 OK created sql table model abduafol4283_analytics.agg_public_holiday .... [[32mSELECT 1[0m in 4.19s]
[0m15:59:15.839525 [debug] [Thread-1  ]: Finished running node model.dbt_data2bots.agg_public_holiday
[0m15:59:15.906205 [debug] [Thread-3  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m15:59:15.911585 [debug] [Thread-3  ]: On model.dbt_data2bots.best_performing_product: COMMIT
[0m15:59:15.912198 [debug] [Thread-3  ]: Using postgres connection "model.dbt_data2bots.best_performing_product"
[0m15:59:15.912639 [debug] [Thread-3  ]: On model.dbt_data2bots.best_performing_product: COMMIT
[0m15:59:16.056134 [debug] [Thread-2  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m15:59:16.061457 [debug] [Thread-2  ]: On model.dbt_data2bots.agg_shipments: COMMIT
[0m15:59:16.062263 [debug] [Thread-2  ]: Using postgres connection "model.dbt_data2bots.agg_shipments"
[0m15:59:16.062827 [debug] [Thread-2  ]: On model.dbt_data2bots.agg_shipments: COMMIT
[0m15:59:16.139379 [debug] [Thread-3  ]: SQL status: COMMIT in 0.0 seconds
[0m15:59:16.142854 [debug] [Thread-3  ]: Using postgres connection "model.dbt_data2bots.best_performing_product"
[0m15:59:16.143296 [debug] [Thread-3  ]: On model.dbt_data2bots.best_performing_product: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.best_performing_product"} */
drop table if exists "d2b_accessment"."abduafol4283_analytics"."best_performing_product__dbt_backup" cascade
[0m15:59:16.270951 [debug] [Thread-2  ]: SQL status: COMMIT in 0.0 seconds
[0m15:59:16.275914 [debug] [Thread-2  ]: Using postgres connection "model.dbt_data2bots.agg_shipments"
[0m15:59:16.276438 [debug] [Thread-2  ]: On model.dbt_data2bots.agg_shipments: /* {"app": "dbt", "dbt_version": "1.6.0", "profile_name": "dbt_data2bots", "target_name": "prod", "node_id": "model.dbt_data2bots.agg_shipments"} */
drop table if exists "d2b_accessment"."abduafol4283_analytics"."agg_shipments__dbt_backup" cascade
[0m15:59:16.406003 [debug] [Thread-3  ]: SQL status: DROP TABLE in 0.0 seconds
[0m15:59:16.428554 [debug] [Thread-3  ]: Timing info for model.dbt_data2bots.best_performing_product (execute): 15:59:11.707090 => 15:59:16.413199
[0m15:59:16.429282 [debug] [Thread-3  ]: On model.dbt_data2bots.best_performing_product: Close
[0m15:59:16.430523 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f24c0769-5b9b-4b87-86cd-a1d45cca206b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc45885a400>]}
[0m15:59:16.431685 [info ] [Thread-3  ]: 3 of 3 OK created sql table model abduafol4283_analytics.best_performing_product  [[32mSELECT 1[0m in 4.79s]
[0m15:59:16.432585 [debug] [Thread-3  ]: Finished running node model.dbt_data2bots.best_performing_product
[0m15:59:16.483111 [debug] [Thread-2  ]: SQL status: DROP TABLE in 0.0 seconds
[0m15:59:16.490852 [debug] [Thread-2  ]: Timing info for model.dbt_data2bots.agg_shipments (execute): 15:59:11.702401 => 15:59:16.490294
[0m15:59:16.493623 [debug] [Thread-2  ]: On model.dbt_data2bots.agg_shipments: Close
[0m15:59:16.496640 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f24c0769-5b9b-4b87-86cd-a1d45cca206b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4791e8fd0>]}
[0m15:59:16.498798 [info ] [Thread-2  ]: 2 of 3 OK created sql table model abduafol4283_analytics.agg_shipments ......... [[32mSELECT 1[0m in 4.85s]
[0m15:59:16.500756 [debug] [Thread-2  ]: Finished running node model.dbt_data2bots.agg_shipments
[0m15:59:16.506664 [debug] [MainThread]: Using postgres connection "master"
[0m15:59:16.507545 [debug] [MainThread]: On master: BEGIN
[0m15:59:16.507978 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:59:21.754786 [debug] [MainThread]: SQL status: BEGIN in 5.0 seconds
[0m15:59:21.757466 [debug] [MainThread]: On master: COMMIT
[0m15:59:21.761695 [debug] [MainThread]: Using postgres connection "master"
[0m15:59:21.763354 [debug] [MainThread]: On master: COMMIT
[0m15:59:21.969947 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m15:59:21.970564 [debug] [MainThread]: On master: Close
[0m15:59:21.971571 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:59:21.972227 [debug] [MainThread]: Connection 'model.dbt_data2bots.agg_public_holiday' was properly closed.
[0m15:59:21.972689 [debug] [MainThread]: Connection 'model.dbt_data2bots.agg_shipments' was properly closed.
[0m15:59:21.973024 [debug] [MainThread]: Connection 'model.dbt_data2bots.best_performing_product' was properly closed.
[0m15:59:21.973438 [info ] [MainThread]: 
[0m15:59:21.973865 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 20.26 seconds (20.26s).
[0m15:59:21.974881 [debug] [MainThread]: Command end result
[0m15:59:21.982993 [info ] [MainThread]: 
[0m15:59:21.983542 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:59:21.983906 [info ] [MainThread]: 
[0m15:59:21.984313 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m15:59:21.985042 [debug] [MainThread]: Command `dbt run` succeeded at 15:59:21.984927 after 20.56 seconds
[0m15:59:21.985489 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4789a6c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc428021c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc458884880>]}
[0m15:59:21.985884 [debug] [MainThread]: Flushing usage events
